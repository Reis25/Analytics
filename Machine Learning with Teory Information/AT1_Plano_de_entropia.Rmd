---
title: "AT1_ plano de entropia e complexidade"
output:
  html_document:
    df_print: paged
---
```{r}
if(!require(gtools)) install.packages("gtools")
```


```{r}

 hc.samples <- function(series, dimension, delay){
  n.series = dim(series)[1]
  hc = matrix(ncol = 2, nrow = n.series)
  for(i in 1:n.series){
    ts = unlist(series[i, ])
    #cat("BP: ", i, "\n")
    probs = bandt.pompe(ts, dimension, delay)
    hc[i, 1] = shannon.entropy.normalized(probs)
    hc[i, 2] = Ccomplexity(probs)
  }
  return(hc)
}
 constant <- function(prob){
  k = (0.5)/length(prob)
  a1 = (0.5 + k) * log(0.5 + k)
  a2 = (length(prob) - 1) * k * log(k)
  a3 = (1 - 0.5) * log(length(prob))
  b = -1/(a1 + a2 + a3)
  return(b)
}

Ccomplexity<-function(prob){
  cc = jensen.divergence(prob) * constant(prob) * shannon.entropy.normalized(prob)
  return(cc)
}
 # Shannon Entropy function -------------------------------------------------------------------------------
shannon.entropy <- function(prob){
  entropy = prob * log(prob)
  entropy[is.nan(entropy)] = 0
  return(-sum(entropy))
}

shannon.entropy.normalized <- function(prob){
  entropy = (shannon.entropy(prob)/log(length(prob)))
  entropy[is.nan(entropy)] = 0
  return(entropy)
}

# Jensen Divergence function ------------------------------------------------------------------------------
jensen.divergence <- function(prob){
  cc = rep(1/length(prob),length(prob))
  s_p = shannon.entropy(prob)
  s_q = shannon.entropy(cc)
  s_pq = shannon.entropy((prob + cc)/2)
  divergence = sum(s_pq - (s_p/2) - (s_q/2))
  return(divergence)
}
define.symbols <- function(D){
  d = c(1:D)
  symbol = matrix(unlist(permutations(n = D, r = D, v = d)),nrow = factorial(D), ncol = D, byrow = FALSE)
  symbol = symbol - 1
  symbol
}


formationPattern <- function(series, D, tau, option){
  
  i = 1
  n = length(series)
  p_patterns = elements = matrix(nrow = n, ncol = D)
  index = c(0:(D-1))
  
  for(s in seq(1, length(series)-(D-1)*tau, by = 1)){
    # the indices for the subsequence
    ind = seq(s, s+(D-1)*tau, by = tau)
    elements[i,] = series[ind]
    p_patterns[i,] = index[order(elements[i,])]
    i = i + 1
  }
  
  if(option == 0){
    p_patterns = na.omit(p_patterns)
    return(p_patterns[1:(i-1),])
  }else if(option == 1){
    elements = na.omit(elements)
    return(elements[1:(i-1),])    
  }
}
bandt.pompe <- function(serie, dimension, delay){  
  fat = factorial(dimension)
  probability = rep(0, fat)
  symbols = define.symbols(dimension)
  p_patterns = formationPattern(serie, dimension, delay, 0)
  n_symbols = dim(p_patterns)[1]
  for(j in 1:n_symbols){
    for(i in 1:fat){
      if(all(p_patterns[j,] == symbols[i,])){ 
        probability[i] = probability[i] + 1
        break
      }
    }
  }
  return(probability/n_symbols)
}

```

```{r}
df <- read.csv("/home/reis/Mestrado Dissertação/Dados/Drugs.csv")
```

```{r}
df
```
```{r}
time_s <- ts(df)
```

```{r}
 a= df[,2]

```


```{r}
plot.ts(df, main="testanto")
```
```{r}
v <- as.array(df)
result_ent <- hc.samples(v[,2], 3 , 1)
```
```{r}
print(dim(time_s))
```

```{r}
hc.samples <- function(series, dimension, delay){
    ts = unlist(series)
    probs = bandt.pompe(ts, dimension, delay)
    h = shannon.entropy.normalized(probs)
    comp = Ccomplexity(probs)
    return(c(h, comp))
}
```


